{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lRqH9IyPPSkI",
   "metadata": {
    "id": "lRqH9IyPPSkI"
   },
   "source": [
    "# Answering the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G3nayPBBPCAh",
   "metadata": {
    "id": "G3nayPBBPCAh"
   },
   "source": [
    "✔️ Answer the questions below (briefly):\n",
    "\n",
    "🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "\n",
    "- The main difference between character n-gram and word n-gram is that character n-gram is based on individual characters, whereas word n-gram is based on whole words\n",
    "- In terms of the OOV (out-of-vocabulary) issue, character n-gram tends to suffer less than word n-gram. This is because character n-gram can capture sub-word information\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "🌈 What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "\n",
    "- the main difference between the two is that stop word removal involves completely eliminating predetermined words that are listed, while stemming involves reducing words to their roots by removing prefixes and suffixes, without removing the entire word.\n",
    "- These techniques can be language-dependent as stop words and word inflections can vary across different languages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "🌈 Is tokenization techniques language dependent? Why?\n",
    "\n",
    "- Yes, tokenization techniques are language-dependent, as different languages have their own unique rules and structures for dividing text into individual units or tokens.\n",
    "\n",
    "\n",
    "🌈 What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "- Count vectorizer is a simple technique that converts a collection of text documents into a matrix of token counts\n",
    "- TF-IDF (term frequency-inverse document frequency) vectorizer, on the other hand, is a more advanced technique that assigns weights to the words based on their importance in the text\n",
    "- it may not be feasible to use all possible n-grams as the number of features can become very large, leading to the curse of dimensionality and computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LhMLKVNVPwKS",
   "metadata": {
    "id": "LhMLKVNVPwKS"
   },
   "source": [
    "# Problem Formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QGaXB_1iP0F5",
   "metadata": {
    "id": "QGaXB_1iP0F5"
   },
   "source": [
    "##problem definition\n",
    "\n",
    "- The problem  is about building a model to classify and detect which real news and fake news from just its titles.\n",
    "\n",
    "- inputs: (60000) observations for a training dataset \n",
    "- output is (59151) label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sd7LKpIoQxEY",
   "metadata": {
    "id": "Sd7LKpIoQxEY"
   },
   "source": [
    "## Function and Model\n",
    "Text preprocessing, Tokenization Then Vectorization on Each text\n",
    "\n",
    "Then Training The Model (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1hXDEg06RVj6",
   "metadata": {
    "id": "1hXDEg06RVj6"
   },
   "source": [
    "## Challenges\n",
    "\n",
    "The Data Has Unclean Text That Contain Huge Number Of Punctuations, Non-English Letters, Misspellings And Grammatical Errors. So it require an appropriate text cleaning technique to be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Suof-y29R5Bg",
   "metadata": {
    "id": "Suof-y29R5Bg"
   },
   "source": [
    "## Model impact\n",
    "\n",
    "Prevent The Spreading of Rumors or False News On Social Media. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MkSuuqeXSE8Z",
   "metadata": {
    "id": "MkSuuqeXSE8Z"
   },
   "source": [
    "## The ideal solution\n",
    "\n",
    "Using a SVC Classifier with GridSearch to get the best hyperparameters combinations to get better Accuracy result (86.23%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdfc8cf",
   "metadata": {
    "id": "ffdfc8cf"
   },
   "source": [
    "# Importing Libraries and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0a7870",
   "metadata": {
    "id": "fa0a7870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-optimize) (21.10.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-optimize) (1.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-optimize) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-optimize) (1.0.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.2.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from xgboost) (1.21.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: joblib in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from seaborn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from seaborn) (1.21.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from seaborn) (3.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dekaito\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize\n",
    "!pip install xgboost\n",
    "!pip install nltk\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5901a0",
   "metadata": {
    "id": "4f5901a0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adbb565",
   "metadata": {
    "id": "5adbb565"
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "df = pd.read_csv(\"xy_train.csv\", index_col='id')\n",
    "test = pd.read_csv('x_test.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5992f87",
   "metadata": {
    "id": "f5992f87",
    "outputId": "76ec4930-fa98-48b3-fb31-32d030cf4aa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a hom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "id                                                              \n",
       "265723  A group of friends began to volunteer at a hom...      0\n",
       "284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
       "207715  In 1961, Goodyear released a kit that allows P...      0\n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
       "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac36fbd",
   "metadata": {
    "id": "fac36fbd",
    "outputId": "c296cbf2-c60a-4f6a-c67a-4f86ed85abe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Date Shape:  (60000, 2)\n",
      "Testing Date Shape:  (59151, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the Shape of the training and test Data\n",
    "print(\"Training Date Shape: \", df.shape)\n",
    "print(\"Testing Date Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af0f46",
   "metadata": {
    "id": "08af0f46"
   },
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BIWq_YOJOS8y",
   "metadata": {
    "id": "BIWq_YOJOS8y"
   },
   "source": [
    "**I Wanted to Use This but was the same as the function Below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b74cdf",
   "metadata": {
    "id": "29b74cdf"
   },
   "outputs": [],
   "source": [
    "# url = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "# user = '@[^\\s]+'\n",
    "# alpha = \"[^a-zA-Z0-9]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74a9a0a",
   "metadata": {
    "id": "e74a9a0a"
   },
   "outputs": [],
   "source": [
    "# #Cleaining The Data\n",
    "# # Drop duplicate rows\n",
    "# df.drop_duplicates(subset='text', inplace=True)\n",
    "# # Remove numbers \n",
    "# df['text'] = df['text'].str.replace('[^A-Za-z]',' ')\n",
    "# # Make sure any double-spaces are single \n",
    "# df['text'] = df['text'].str.replace('  ',' ')\n",
    "# # Transform all text to lowercase\n",
    "# df['text'] = df['text'].str.lower()\n",
    "\n",
    "\n",
    "# df['text'] = df['text'].str.replace(\"don't\", \"do not\")\n",
    "# df['text'] = df['text'].str.replace(\"won't\", \"will not\")\n",
    "# df['text'] = df['text'].str.replace(\"'re\", \" are\")\n",
    "# df['text'] = df['text'].str.replace(\"i'm\", \"i am\")\n",
    "# df['text'] = df['text'].str.replace(\"'m\", \" am\")\n",
    "# df['text'] = df['text'].str.replace(\"let's\", \"let us\")\n",
    "# df['text'] = df['text'].str.replace(\"'s\", \" is\")\n",
    "# df['text'] = df['text'].str.replace(\"'ve\", \" have\")\n",
    "# df['text'] = df['text'].str.replace(\"can't\", \"can not\")\n",
    "# df['text'] = df['text'].str.replace(\"shan't\", \"shall not\")\n",
    "# df['text'] = df['text'].str.replace(\"n't\", \" not\")\n",
    "# df['text'] = df['text'].str.replace(\"'d\", \" would\")\n",
    "# df['text'] = df['text'].str.replace(\"'ll\", \" will\")\n",
    "# df['text'] = df['text'].str.replace(url, \"URL\")\n",
    "# df['text'] = df['text'].str.replace(user, \" \")\n",
    "# df['text'] = df['text'].str.replace(alpha, \" \")\n",
    "# df['text'] = df['text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b49852",
   "metadata": {
    "id": "88b49852",
    "outputId": "57ce2f3b-bf92-46eb-983a-2e40a5d5b70e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.536200\n",
       "1    0.459933\n",
       "2    0.003867\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking The Label Class\n",
    "df[\"label\"].value_counts(normalize=True)\n",
    "# We can see that there are Label 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16923b7a",
   "metadata": {
    "id": "16923b7a"
   },
   "outputs": [],
   "source": [
    "# We drop the ones that Contain Label 2\n",
    "df.drop(df[df.label == 2].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11386a96",
   "metadata": {
    "id": "11386a96",
    "outputId": "767d270d-a334-43ef-bb7f-a77703f5b9b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We Can See that the data is Good now\n",
    "sns.countplot(x=\"label\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301b9c53",
   "metadata": {
    "id": "301b9c53",
    "outputId": "35110f9d-1d44-4378-8af9-d95e063ba8a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.538281\n",
       "1    0.461719\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34607052",
   "metadata": {
    "id": "34607052"
   },
   "source": [
    "## Cleaning The Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881dca2e",
   "metadata": {
    "id": "881dca2e"
   },
   "source": [
    "**We Will Clean The Data and use Lemmatization and Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e742aaf1",
   "metadata": {
    "id": "e742aaf1",
    "outputId": "10233cec-8d3b-463d-fc54-8ded733a6e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'needn', 's', \"you've\", 'ain', 'did', 'to', 'shouldn', 'had', \"you'd\", 'too', 'be', 'can', \"hadn't\", 'ma', \"don't\", 'nor', 'does', 'ourselves', 'am', 'down', \"aren't\", 'by', \"won't\", 'such', 'haven', 'not', 'won', 'just', 'them', 'it', 'but', 'of', 'having', 'i', 'me', 'my', 'isn', 'mightn', 'herself', 't', 'at', \"shan't\", 'you', 'are', 'under', 'once', 'will', 'himself', 'o', 'weren', 'y', 'myself', 'then', 'doesn', 'hasn', \"mustn't\", 'out', 'wouldn', 'few', 'themselves', 'only', 'both', \"couldn't\", 'so', 'on', 'up', 'why', 'don', 'were', 'and', 'with', 'should', \"mightn't\", 'his', 'which', 'she', 'above', 'here', 'very', \"hasn't\", 'through', 'been', 'yours', 'some', \"she's\", \"should've\", 'now', 'how', 'who', 'off', 'when', 'while', 'below', 'yourselves', 'ours', 'hers', \"that'll\", 'll', 'm', 'from', 've', 'is', 'same', 'because', 'most', 'her', 'any', 'aren', 'do', 'being', \"shouldn't\", 'own', 'theirs', 'against', \"doesn't\", 'your', 'couldn', 'itself', 'again', 'for', 'about', 'each', 'before', \"haven't\", 'shan', 're', 'that', 'those', \"needn't\", 'what', 'this', 'until', 'doing', 'if', \"isn't\", \"wasn't\", 'a', \"you'll\", 'in', 'has', 'its', 'between', 'more', 'where', \"wouldn't\", 'as', 'we', 'or', 'yourself', \"didn't\", 'than', 'they', 'other', 'wasn', \"it's\", 'there', 'after', 'whom', 'our', 'further', 'into', 'he', 'him', 'an', 'hadn', 'the', \"weren't\", 'mustn', 'was', 'all', 'have', 'didn', \"you're\", 'their', 'these', 'during', 'no', 'over', 'd'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DeKaiTo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DeKaiTo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DeKaiTo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DeKaiTo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "\n",
    "def cleaning_text(text, for_embedding):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "\n",
    "    \"\"\"\n",
    "    #match one or more white sepace\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    #match <any num of words>\n",
    "    RE_TAGS = re.compile(r\"<.*?>\")\n",
    "    #match any English word\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž0-9]+\", re.IGNORECASE)\n",
    "    #match any word with word boundary.\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b^[^A-Za-zÀ-ž0-9]+\\b\", re.IGNORECASE)\n",
    "    \n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        \n",
    "        #match any English word and any punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        #match any word and any punctuation with word boundary.\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    #remove one or more white sepace\n",
    "    text = re.sub(RE_TAGS, \" \", text) \n",
    "    #remove <any num of words>\n",
    "    text = re.sub(RE_ASCII, \" \", text) \n",
    "    #remove any English word\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    #remove any word with word boundary.\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "def lemmatize_clean_text(text ,for_embedding=False):\n",
    "\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and lemmatize\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    word_tokens = cleaning_text(text, for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "def stemming_clean_text(text ,for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemming\n",
    "    \"\"\"\n",
    "\n",
    "    word_tokens = cleaning_text(text,for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [stemmer.stem(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960e171d",
   "metadata": {
    "id": "960e171d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    A group of friends began to volunteer at a hom...\n",
       "284269    British Prime Minister @Theresa_May on Nerve A...\n",
       "207715    In 1961, Goodyear released a kit that allows P...\n",
       "551106    Happy Birthday, Bob Barker! The Price Is Right...\n",
       "8584      Obama to Nation: 聙\"Innocent Cops and Unarmed Y...\n",
       "                                ...                        \n",
       "70046     Finish Sniper Simo H盲yh盲 during the invasion o...\n",
       "189377    Nigerian Prince Scam took $110K from Kansas ma...\n",
       "93486     Is It Safe To Smoke Marijuana During Pregnancy...\n",
       "140950    Julius Caesar upon realizing that everyone in ...\n",
       "34509     Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...\n",
       "Name: text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Before Applying the Function\n",
    "df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ef30b3",
   "metadata": {
    "id": "80ef30b3"
   },
   "outputs": [],
   "source": [
    "# Applying the Function for both Test and Training Dataset\n",
    "df_lemmatized = df[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## clean and lemmatiz training set\n",
    "df_stemmed = df[\"text\"].map(lambda x: stemming_clean_text(x, for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and stemming training set\n",
    "test_lemmatized = test[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set\n",
    "test_stemmed = test[\"text\"].map(lambda x: stemming_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658543ec",
   "metadata": {
    "id": "658543ec",
    "outputId": "38316583-6fce-446f-c700-e61a65d1db82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group friend began volunteer homeless shelter ...\n",
       "284269    british prime minister theresa may nerve attac...\n",
       "207715    1961 goodyear released kit allows ps2s brought...\n",
       "551106    happy birthday bob barker price right host lik...\n",
       "8584      obama nation innocent cop unarmed young black ...\n",
       "                                ...                        \n",
       "70046     finish sniper simo h yh invasion finland ussr ...\n",
       "189377    nigerian prince scam took 110k kansa man 10 ye...\n",
       "93486       safe smoke marijuana pregnancy surprised answer\n",
       "140950    julius caesar upon realizing everyone room kni...\n",
       "34509     jeff bridge releasing leeping tape new album d...\n",
       "Name: text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data After Applying the Function\n",
    "df_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fd19b7",
   "metadata": {
    "id": "c8fd19b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group friend began volunt homeless shelter nei...\n",
       "284269    british prime minist theresa may nerv attack f...\n",
       "207715    1961 goodyear releas kit allow ps2s brought he...\n",
       "551106    happi birthday bob barker price right host lik...\n",
       "8584      obama nation innoc cop unarm young black men d...\n",
       "                                ...                        \n",
       "70046     finish sniper simo h yh invas finland ussr 193...\n",
       "189377    nigerian princ scam took 110k kansa man 10 yea...\n",
       "93486          safe smoke marijuana pregnanc surpris answer\n",
       "140950    julius caesar upon realiz everyon room knife e...\n",
       "34509     jeff bridg releas leep tape new album design h...\n",
       "Name: text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Stem Also\n",
    "df_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37886769",
   "metadata": {
    "id": "37886769",
    "outputId": "7407c89f-e0de-4e83-d027-500aa2c8155e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            4307\n",
       "year         4121\n",
       "one          3285\n",
       "new          2998\n",
       "like         2949\n",
       "man          2705\n",
       "trump        2577\n",
       "u            2513\n",
       "colorized    2430\n",
       "people       2315\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in Traing data After lemmatization\n",
    "word_freq_lemmatized = pd.Series(\" \".join(df_lemmatized).split()).value_counts()\n",
    "word_freq_lemmatized[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc12fc7",
   "metadata": {
    "id": "2dc12fc7",
    "outputId": "de97d8ee-e1c9-4291-c917-6cec4b6133ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4293\n",
       "year     4125\n",
       "one      3285\n",
       "like     3128\n",
       "new      2998\n",
       "look     2847\n",
       "color    2737\n",
       "man      2728\n",
       "get      2602\n",
       "trump    2578\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in in Traing data After Stemming\n",
    "word_freq_stemmed = pd.Series(\" \".join(df_stemmed).split()).value_counts()\n",
    "word_freq_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21fb64",
   "metadata": {
    "id": "9b21fb64"
   },
   "source": [
    "# Part 2: Building The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca67f34",
   "metadata": {
    "id": "eca67f34"
   },
   "source": [
    "## A Tunable Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e708630e",
   "metadata": {
    "id": "e708630e",
    "outputId": "41501830-b3bd-42a6-fe5c-290c48e79f80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature creation and modelling\n",
    "\n",
    "pipe_lg = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()), \n",
    "        (\"lg\", LogisticRegression(max_iter=10000, random_state = 44, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_xgb = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"xgb\", XGBClassifier(random_state=44, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_svc = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"svc\", SVC(random_state=44, probability=True))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc9fb3",
   "metadata": {
    "id": "83cc9fb3"
   },
   "source": [
    "## Here I Will Do 5 Trials,\n",
    "\n",
    "**Using 3 Models Which Are Logistic Regression, XGBoost And SVM.**\n",
    "\n",
    "\n",
    "**Each Model I Will Use Stemmed Data And another Trial I Will Use Lemmatized Data Except For SVM will only Use Stemmed Because The Long Time it takes toy train the SVM Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0dade",
   "metadata": {
    "id": "3fb0dade"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816b01a",
   "metadata": {
    "id": "3816b01a"
   },
   "source": [
    "### Trial 1: Using Stemmed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "789944a9",
   "metadata": {
    "id": "789944a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('lg',\n",
       "                                              LogisticRegression(max_iter=10000,\n",
       "                                                                 n_jobs=-1,\n",
       "                                                                 random_state=44))]),\n",
       "                   n_iter=200, n_jobs=-1,\n",
       "                   param_distributions={'lg__C': [1.5, 2.0, 3.5],\n",
       "                                        'lg__class_weight': ['balanced', None],\n",
       "                                        'lg__fit_intercept': [False, True],\n",
       "                                        'lg__solver': ['sag', 'newton-cg',\n",
       "                                                       'lbfgs', 'liblinear',\n",
       "                                                       'saga'],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [6],\n",
       "                                        'tfidf__ngram_range': [(1, 4)],\n",
       "                                        'tfidf__smooth_idf': [True],\n",
       "                                        'tfidf__strip_accents': [None],\n",
       "                                        'tfidf__sublinear_tf': [True]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters To Use\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True],\n",
    "    'tfidf__strip_accents':[None],\n",
    "    'tfidf__smooth_idf':[True],\n",
    "    'tfidf__ngram_range': [(1, 4)],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'tfidf__min_df': [6], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga'],\n",
    "    'lg__C': [1.5,2.0,3.5],\n",
    "    'lg__fit_intercept':[False, True]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Here we Will use The Stemmed Data\n",
    "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=2, scoring=\"roc_auc\", n_iter=200, verbose=1)\n",
    "pipe_lg_clf.fit(df_stemmed, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09ecad69",
   "metadata": {
    "id": "09ecad69",
    "outputId": "c0eadc23-1bd1-41a7-a172-2a207a3478f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8608721069334946\n",
      "best params {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'lg__solver': 'newton-cg', 'lg__fit_intercept': False, 'lg__class_weight': 'balanced', 'lg__C': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6cc4468",
   "metadata": {
    "id": "f6cc4468",
    "outputId": "a165387a-bee6-400e-d22b-8d74aa3e8c58"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_stemmed.index\n",
    "submission['label'] = pipe_lg_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('lg_stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301170c",
   "metadata": {
    "id": "4301170c"
   },
   "source": [
    "### Trial 2: Using Lemmatized Data And GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aabfdb69",
   "metadata": {
    "id": "aabfdb69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 72 candidates, totalling 144 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('lg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           n_jobs=-1,\n",
       "                                                           random_state=44))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lg__C': [1.5, 2.0, 3.5],\n",
       "                         'lg__class_weight': ['balanced', None],\n",
       "                         'lg__fit_intercept': [False, True],\n",
       "                         'lg__solver': ['sag', 'newton-cg', 'lbfgs',\n",
       "                                        'liblinear', 'saga', 'sag'],\n",
       "                         'tfidf__analyzer': ['word'], 'tfidf__max_df': [0.2],\n",
       "                         'tfidf__min_df': [5], 'tfidf__ngram_range': [(1, 4)],\n",
       "                         'tfidf__smooth_idf': [False],\n",
       "                         'tfidf__strip_accents': ['unicode'],\n",
       "                         'tfidf__sublinear_tf': [False]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters To Use\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[False],\n",
    "    'tfidf__strip_accents':[\"unicode\"],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 4)],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'tfidf__min_df': [5], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga', 'sag'],\n",
    "    'lg__C': [1.5,2.0,3.5],\n",
    "    'lg__fit_intercept':[False, True],\n",
    "\n",
    "}\n",
    "\n",
    "# Here we Will use The Lemmetized Data\n",
    "pipe_lg_clf2 = GridSearchCV(pipe_lg, params, n_jobs=-1,cv=2, scoring=\"roc_auc\", verbose=1)\n",
    "pipe_lg_clf2.fit(df_lemmatized, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdcd38dd",
   "metadata": {
    "id": "bdcd38dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8628684361111858\n",
      "best params {'lg__C': 2.0, 'lg__class_weight': 'balanced', 'lg__fit_intercept': False, 'lg__solver': 'liblinear', 'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 4), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': False}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf2.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bcbddaa",
   "metadata": {
    "id": "2bcbddaa"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_lemmatized.index\n",
    "submission['label'] = pipe_lg_clf2.predict_proba(test_lemmatized)[:,1]\n",
    "submission.to_csv('lg_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebea684",
   "metadata": {
    "id": "2ebea684"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9088515",
   "metadata": {
    "id": "d9088515"
   },
   "source": [
    "### Trial 3: Using Stemmed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5efc403",
   "metadata": {
    "id": "e5efc403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_t...\n",
       "                   param_distributions={'tfidf__analyzer': ['char'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [6],\n",
       "                                        'tfidf__ngram_range': [(1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': ['unicode'],\n",
       "                                        'tfidf__sublinear_tf': [False],\n",
       "                                        'xgb__booster': ['dart'],\n",
       "                                        'xgb__gamma': [0.1],\n",
       "                                        'xgb__learning_rate': [0.1],\n",
       "                                        'xgb__max_depth': [60, 65],\n",
       "                                        'xgb__n_estimators': [450, 500],\n",
       "                                        'xgb__subsample': [0.8]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters To Use\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[False], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5)],\n",
    "    'tfidf__min_df': [6], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['char'],\n",
    "    'xgb__booster':['dart'],\n",
    "    'xgb__n_estimators': [450, 500],\n",
    "    'xgb__max_depth': [60, 65],\n",
    "    'xgb__learning_rate': [0.1],\n",
    "    'xgb__gamma': [0.1],\n",
    "    'xgb__subsample': [0.8]\n",
    "\n",
    "}\n",
    "\n",
    "# Here we Will use The Stemmed Data\n",
    "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=2, scoring=\"roc_auc\", n_iter=10, verbose=1)\n",
    "pipe_xgb_clf.fit(df_stemmed, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d53a11a9",
   "metadata": {
    "id": "d53a11a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8495079273589194\n",
      "best score {'xgb__subsample': 0.8, 'xgb__n_estimators': 450, 'xgb__max_depth': 65, 'xgb__learning_rate': 0.1, 'xgb__gamma': 0.1, 'xgb__booster': 'dart', 'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
    "print('best score {}'.format(pipe_xgb_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d618c24",
   "metadata": {
    "id": "8d618c24"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_stemmed.index\n",
    "submission['label'] = pipe_xgb_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('xgb_stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f1af3",
   "metadata": {
    "id": "8a3f1af3"
   },
   "source": [
    "### Trial 4: Using Lemmatized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fe9c896",
   "metadata": {
    "id": "5fe9c896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_t...\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [6],\n",
       "                                        'tfidf__ngram_range': [(1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': ['unicode'],\n",
       "                                        'tfidf__sublinear_tf': [False],\n",
       "                                        'xgb__booster': ['gbtree', 'gblinear',\n",
       "                                                         'dart'],\n",
       "                                        'xgb__gamma': [0.1],\n",
       "                                        'xgb__learning_rate': [0.1],\n",
       "                                        'xgb__max_depth': [60, 70],\n",
       "                                        'xgb__n_estimators': [450, 500],\n",
       "                                        'xgb__subsample': [0.8]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameters To Use\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[False], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5)],\n",
    "    'tfidf__min_df': [6], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['char'],\n",
    "    'xgb__booster':['gbtree','gblinear', 'dart'],\n",
    "    'xgb__n_estimators': [450, 500],\n",
    "    'xgb__max_depth': [60, 70],\n",
    "    'xgb__learning_rate': [0.1],\n",
    "    'xgb__gamma': [0.1],\n",
    "    'xgb__subsample': [0.8]\n",
    "\n",
    "}\n",
    "\n",
    "# Here we Will use The Lemmatized Data\n",
    "pipe_xgb_clf2 = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=2, scoring=\"roc_auc\", n_iter=10, verbose=1)\n",
    "pipe_xgb_clf2.fit(df_lemmatized, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c40c7a8a",
   "metadata": {
    "id": "c40c7a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8508863791297021\n",
      "best params {'xgb__subsample': 0.8, 'xgb__n_estimators': 450, 'xgb__max_depth': 60, 'xgb__learning_rate': 0.1, 'xgb__gamma': 0.1, 'xgb__booster': 'dart', 'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf2.best_score_))\n",
    "print('best params {}'.format(pipe_xgb_clf2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b7e0885",
   "metadata": {
    "id": "8b7e0885"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_stemmed.index\n",
    "submission['label'] = pipe_xgb_clf2.predict_proba(test_lemmatized)[:,1]\n",
    "submission.to_csv('xgb_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4bd364",
   "metadata": {
    "id": "8a4bd364"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27db26",
   "metadata": {
    "id": "9f27db26"
   },
   "source": [
    "### Trial 5: Using Stemmed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faf27bc2",
   "metadata": {
    "id": "faf27bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('svc',\n",
       "                                        SVC(probability=True,\n",
       "                                            random_state=44))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tfidf__analyzer': ['word'], 'tfidf__max_df': [0.2],\n",
       "                         'tfidf__min_df': [5], 'tfidf__ngram_range': [(1, 2)],\n",
       "                         'tfidf__smooth_idf': [False],\n",
       "                         'tfidf__strip_accents': ['unicode'],\n",
       "                         'tfidf__sublinear_tf': [True]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define parameter space to test\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__min_df': [5], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['word']\n",
    "}\n",
    "\n",
    "# Here we Will use The Stemmed Data\n",
    "pipe_svc_clf = GridSearchCV(pipe_svc, params, n_jobs=-1,cv=2, scoring=\"roc_auc\", verbose=1)\n",
    "pipe_svc_clf.fit(df_stemmed, df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11ea6e21",
   "metadata": {
    "id": "11ea6e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8623373241850132\n",
      "best score {'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_svc_clf.best_score_))\n",
    "print('best score {}'.format(pipe_svc_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4564a48",
   "metadata": {
    "id": "a4564a48"
   },
   "outputs": [],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_stemmed.index\n",
    "submission['label'] = pipe_svc_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('svc_stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf0985",
   "metadata": {
    "id": "4baf0985"
   },
   "source": [
    "# Trials Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b07d3",
   "metadata": {
    "id": "2b1b07d3"
   },
   "source": [
    "**As we Can see that Lemmatization is Better than Stemmed Data and the Reason is That:**\n",
    "\n",
    "\n",
    "**Stemming is a crude process that chops off the ends of words, while lemmatization is a more sophisticated process that uses a vocabulary and morphological analysis to find the dictionary form of a word.**\n",
    "\n",
    "**That aside let's Compare between the models**\n",
    "\n",
    "**We can See that the Accuracy of Each Model isn't that big but compared to the time taken the XGBoost and SVC took to much time (More that 12 Hours) and this with hust few hyperparameters**\n",
    "\n",
    "**But For the SVC I Used only a few Hyperprameters and yet the accuracy with better than the other models so If I Did include more Hyperparameters Range I Think it will be even more better But the time will Skyrocket**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lRqH9IyPPSkI",
    "LhMLKVNVPwKS",
    "ffdfc8cf",
    "08af0f46",
    "34607052",
    "9b21fb64",
    "eca67f34",
    "3fb0dade",
    "3816b01a",
    "4301170c",
    "2ebea684",
    "d9088515",
    "8a3f1af3",
    "8a4bd364",
    "9f27db26",
    "4baf0985"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
